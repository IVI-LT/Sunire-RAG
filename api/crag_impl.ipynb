{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing CRAGS #\n",
    "\n",
    " - Load URLS\n",
    " - Retriever \n",
    " - Get relevant data\n",
    " - Check top data, if not ideal perform a web search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### API Keys ###\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# (optional) LangSmith to inspect inside your chain or agent.\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Model ###\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_together import ChatTogether\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# client = Together(api_key = os.getenv(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "#load model\n",
    "model = ChatTogether(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    temperature=0,\n",
    "    max_tokens=320,\n",
    "    # top_k=50,\n",
    "    together_api_key= os.getenv(\"TOGETHER_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_together import TogetherEmbeddings\n",
    "\n",
    "\n",
    "### Create Index using an the data folder:\n",
    "file_url = open('/data/urls/links.txt')\n",
    "url_txt = file_url.read()\n",
    "urls = url_txt.split(\"\\n\")\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "# check if docs loaded\n",
    "#print(docs[2]) \n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, \n",
    "                                    embedding = TogetherEmbeddings(model=\"togethercomputer/m2-bert-80M-8k-retrieval\")\n",
    "                                    )\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grades the document until relevant ###\n",
    "\n",
    " - choose question \n",
    " - retrieve relevant docs, choose the top one\n",
    " - check relevancy of top n\n",
    " - pass rel_docs to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final binary_score='no'\n",
      "final binary_score='no'\n",
      "final binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatTogether(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    temperature=0,\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# total 443\n",
    "# question = \"What are the two common types of skin cancer? What is Non Melanoma\"\n",
    "\n",
    "# testing websearch , question has no relevant docs\n",
    "question = \"is Cryosurgery invasive\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "rel_docs = ''\n",
    "\n",
    "for i in range(min(3, len(docs))):\n",
    "    doc_txt = docs[i].page_content\n",
    "    final = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}) \n",
    "    print(\"final \" + str(final)) \n",
    "    if(str(final) == \"binary_score='yes'\" ):\n",
    "        rel_docs=doc_txt\n",
    "        # print(final)\n",
    "        break\n",
    "\n",
    "\n",
    "# no relevant docs then perform a websearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced question: ###\n",
    "\n",
    "**Input**   \n",
    "\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: What are the two common types of skin cancer? What is Non Melanoma \n",
    "Context: lump is often tender to touch and may bleed. It may develop into an ulcer.Pre-cancerous growthsThere are pre-cancerous growths that may develop into non-melanoma skin cancer if they are not treated.These include:Bowen's diseaseActinic keratosesBowen's diseaseBowen's disease is also called squamous cell carcinoma in situ. It develops slowly and is easy to treat.The main sign is a red, scaly patch on the skin. It may be itchy. It can appear on any area of the skin. In women, it is often found on the lower legs.It can develop into SCC.Actinic keratosesActinic keratoses are dry, scaly patches of skin. They are also known as solar keratoses. They are caused by the skin being exposed to the sun for a long time.You may have a lot of these patches. They can be pink, red or brown. They can vary in size from a few millimetres across to a few centimetres.They often appear on parts of your body that have been in the sun a lot. For example, the back of your hands or your scalp.The affected skin can sometimes become very thick. The patches can look like small horns or spikes.There is a small risk of them developing into SCC.Diagnosing non-melanoma skin cancerYour GP can examine your skin for signs of skin   \n",
    "\n",
    "**Output**   \n",
    "\n",
    "The two common types of skin cancer are Melanoma and Non-Melanoma. Non-Melanoma skin cancer is further divided into two main types: Basal Cell Carcinoma and Squamous Cell Carcinoma (SCC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Is the doc relevant, \n",
    " - else perform a web search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from websearch import WebSearch\n",
    "import io\n",
    "import requests\n",
    "from PyPDF2  import PdfReader\n",
    "\n",
    "\n",
    "# convert pdf to text\n",
    "def pdf_2_text(url):\n",
    "    print(url.page_content)\n",
    "    r = requests.get(url.page_content)\n",
    "    f = io.BytesIO(r.content)\n",
    "\n",
    "    reader = PdfReader(f)\n",
    "    contents = reader.getPage(0).extractText().split('\\n')\n",
    "    return contents\n",
    "\n",
    "# define web-search\n",
    "def web_search(question):\n",
    "    web = WebSearch(question)\n",
    "    pdfs = web.pdf\n",
    "    search_urls = [pdf for pdf in pdfs[:5]]\n",
    "    # print(\"urls: \" + urls[0])\n",
    "\n",
    "    search_docs = [pdf_2_text(WebBaseLoader(url).load())for url in search_urls]\n",
    "\n",
    "    search_docs_list = [item for sublist in search_docs for item in sublist]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=300, chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    search_doc_splits = text_splitter.split_documents(search_docs_list)\n",
    "    ## print(doc_splits)\n",
    "    return search_doc_splits\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here now\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere now\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     rag_in \u001b[38;5;241m=\u001b[39m \u001b[43mweb_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(rag_in)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#indexing \u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#vectorstore = FAISS.from_documents(documents=rag_in, \u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#                                embedding = TogetherEmbeddings(model=\"togethercomputer/m2-bert-80M-8k-retrieval\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#output = rag_chain.invoke({\"context\": retriever, \"question\": question})\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#print(output)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 27\u001b[0m, in \u001b[0;36mweb_search\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     24\u001b[0m search_urls \u001b[38;5;241m=\u001b[39m [pdf \u001b[38;5;28;01mfor\u001b[39;00m pdf \u001b[38;5;129;01min\u001b[39;00m pdfs[:\u001b[38;5;241m5\u001b[39m]]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(\"urls: \" + urls[0])\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m search_docs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpdf_2_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWebBaseLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msearch_urls\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     29\u001b[0m search_docs_list \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m search_docs \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m     31\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter\u001b[38;5;241m.\u001b[39mfrom_tiktoken_encoder(\n\u001b[0;32m     32\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n",
      "Cell \u001b[1;32mIn[15], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m search_urls \u001b[38;5;241m=\u001b[39m [pdf \u001b[38;5;28;01mfor\u001b[39;00m pdf \u001b[38;5;129;01min\u001b[39;00m pdfs[:\u001b[38;5;241m5\u001b[39m]]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(\"urls: \" + urls[0])\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m search_docs \u001b[38;5;241m=\u001b[39m [\u001b[43mpdf_2_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWebBaseLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_urls]\n\u001b[0;32m     29\u001b[0m search_docs_list \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m search_docs \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m     31\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter\u001b[38;5;241m.\u001b[39mfrom_tiktoken_encoder(\n\u001b[0;32m     32\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36mpdf_2_text\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpdf_2_text\u001b[39m(url):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43murl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m)\n\u001b[0;32m     13\u001b[0m     r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url\u001b[38;5;241m.\u001b[39msource)\n\u001b[0;32m     14\u001b[0m     f \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(r\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'source'"
     ]
    }
   ],
   "source": [
    "### generates the prompt\n",
    "\n",
    "# If there is no docs then perform web search\n",
    "if(rel_docs != ''):\n",
    "    #print(\"here\")\n",
    "    output = rag_chain.invoke({\"context\": rel_docs, \"question\": question})\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"here now\")\n",
    "    rag_in = web_search(question)\n",
    "    print(rag_in)\n",
    "    #indexing \n",
    "    #vectorstore = FAISS.from_documents(documents=rag_in, \n",
    "    #                                embedding = TogetherEmbeddings(model=\"togethercomputer/m2-bert-80M-8k-retrieval\")\n",
    "    #                                )\n",
    "#\n",
    "    #retriever = vectorstore.as_retriever()\n",
    "\n",
    "    #output = rag_chain.invoke({\"context\": retriever, \"question\": question})\n",
    "    #print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is Cryosurgery invasive'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web = WebSearch(question)\n",
    "pdfs = web.pdf\n",
    "search_urls = [pdf for pdf in pdfs[:5]]\n",
    "\n",
    "check = search_urls[0]\n",
    "pdf_loaded = WebBaseLoader(check).load()\n",
    "print(pdf_loaded[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_loaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpdf_loaded\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#r = requests.get(pdf_loaded)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#f = io.BytesIO(r.content)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# search_doc_splits = text_splitter.split_documents(search_docs_list)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pdf_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "print(pdf_loaded[0].page_content)\n",
    "\n",
    "#r = requests.get(pdf_loaded)\n",
    "#f = io.BytesIO(r.content)\n",
    "#\n",
    "#reader = PdfReader(f)\n",
    "#contents = reader.getPage(0).extractText().split('\\n')\n",
    "\n",
    "# search_docs = [pdf_2_text(WebBaseLoader(url).load())for url in search_urls]\n",
    "# search_docs_list = [item for sublist in search_docs for item in sublist]\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     chunk_size=300, chunk_overlap=50\n",
    "# )\n",
    "# search_doc_splits = text_splitter.split_documents(search_docs_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
